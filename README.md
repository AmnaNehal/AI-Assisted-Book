
# ğŸ“š AI-Assisted Book â€” Production-Ready RAG Chatbot

A full-stack Retrieval-Augmented Generation (RAG) chatbot system that enables users to ask contextual questions about a book and receive grounded, accurate answers powered by semantic vector search and LLM generation.

Designed with production deployment, scalability, and clean architecture principles in mind.

---

## ğŸ” Project Overview

This system combines:

* ğŸ” **Semantic search** using Qdrant vector database
* ğŸ§  **LLM-based answer generation** via Cohere
* âš¡ **Async FastAPI backend**
* ğŸŒ **Modern React + Docusaurus frontend**
* â˜ï¸ **Cloud deployment (Render + Vercel)**

The chatbot ensures responses are grounded in retrieved book content, minimizing hallucinations and improving answer reliability.

---

## ğŸ— Architecture

```
User Question
      â†“
Cohere Embedding (embed-english-v3.0)
      â†“
Qdrant Vector Search
      â†“
Top-K Relevant Chunks
      â†“
Prompt Construction (Context + Query)
      â†“
Cohere Chat Model
      â†“
Validated, Grounded Response
```

---

## âš™ï¸ Tech Stack

### Backend

* **FastAPI (Python 3.11)**
* Async SQLAlchemy
* Cohere v5 API
* Qdrant Cloud (vector database)
* Neon PostgreSQL (conversation persistence)

### Frontend

* React
* Docusaurus
* TypeScript
* Modular component architecture

### Deployment

* Backend: Render
* Frontend: Vercel
* Environment-based configuration
* CORS-managed API access

---

## ğŸ§  Core Engineering Highlights

### âœ… Retrieval-Augmented Generation (RAG)

* Query embedding generation
* Semantic similarity search
* Context-aware prompt engineering
* Response validation layer

### âœ… Async-first Backend

* Fully async request handling
* Non-blocking database access
* Clean separation of concerns:

  * `core/` (RAG orchestration)
  * `api/` (routes)
  * `db/` (data layer)
  * `utils/` (logging & validation)

### âœ… Production Deployment Ready

* Environment variable configuration
* Proper `.gitignore`
* Cloud vector DB integration
* CORS configuration for multi-origin deployment

### âœ… Frontend Engineering

* Scrollable chat UI
* Typing indicator
* Auto-scroll behavior
* Clean modular styling
* Environment-based backend switching

---

## ğŸ“ Project Structure

```
backend/
  backend_rag/
    api/
    core/
    db/
    config/
    utils/
    main.py

frontend_book/
  src/
    components/RagChatbot/
    theme/Layout/
```

---

## ğŸš€ Performance Optimizations

* Reduced retrieval chunk count
* Controlled max token generation
* Optional embedding caching
* Async database usage
* Clean prompt construction

---

## ğŸ” Security & Environment Handling

* No secrets committed
* Environment-based configuration
* Separate dev and production backend URLs
* Proper CORS management

---

## ğŸ“¡ API Endpoint

### POST `/api/query-global`

```json
{
  "query": "What is ROS 2 nervous system?",
  "conversation_id": "default_conversation"
}
```

Returns:

* Generated answer
* Retrieved chunks
* Validation result
* Query metadata

---

## ğŸ“ˆ Future Improvements

* Streaming responses
* Re-ranking layer
* Vector cache layer
* Token usage analytics
* Multi-book indexing

---

## ğŸ’¼ Resume Value

This project demonstrates:

* End-to-end full-stack development
* LLM integration in production context
* Vector database implementation
* Cloud deployment workflows
* Async Python architecture
* Scalable modular code structure
* Real-world AI system engineering

---

## ğŸ‘¨â€ğŸ’» Author

Full-stack AI Engineer focused on scalable RAG systems, production-ready LLM integrations, and modern cloud deployment architecture.


